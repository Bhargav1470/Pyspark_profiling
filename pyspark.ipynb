{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "\u001b[33m  DEPRECATION: pyspark is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed py4j-0.10.9.7 pyspark-3.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+---------------------+---------+--------+-----------+-----------+------------------+------------------+--------+-------+----------+--------+-------+----------+\n",
      "|Name_count|Age_count|Salary_count|Date_of_joining_count|Name_mean|Age_mean|Salary_mean|Name_stddev|Age_stddev        |Salary_stddev     |Name_min|Age_min|Salary_min|Name_max|Age_max|Salary_max|\n",
      "+----------+---------+------------+---------------------+---------+--------+-----------+-----------+------------------+------------------+--------+-------+----------+--------+-------+----------+\n",
      "|10        |10       |10          |10                   |NULL     |37.9    |43316.6    |NULL       |11.454741861391339|11282.762469064628|Craig   |23     |24560.0   |smith   |56     |61000.0   |\n",
      "+----------+---------+------------+---------------------+---------+--------+-----------+-----------+------------------+------------------+--------+-------+----------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, mean, stddev, min, max\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkProfile\").getOrCreate()\n",
    "\n",
    "# Load your dataset into a PySpark DataFrame\n",
    "# Replace 'your_dataset.csv' with your actual dataset file\n",
    "df_spark = spark.read.csv('newdata.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Display basic statistics from the PySpark DataFrame, excluding Date_of_joining\n",
    "df_stats = df_spark.select(\n",
    "    *[count(col(c)).alias(c + \"_count\") for c in df_spark.columns],\n",
    "    *[mean(col(c)).alias(c + \"_mean\") for c in df_spark.columns if c != \"Date_of_joining\"],\n",
    "    *[stddev(col(c)).alias(c + \"_stddev\") for c in df_spark.columns if c != \"Date_of_joining\"],\n",
    "    *[min(col(c)).alias(c + \"_min\") for c in df_spark.columns if c != \"Date_of_joining\"],\n",
    "    *[max(col(c)).alias(c + \"_max\") for c in df_spark.columns if c != \"Date_of_joining\"]\n",
    ")\n",
    "\n",
    "# Show the statistics\n",
    "df_stats.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16543/3630016228.py:2: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  from pandas_profiling import ProfileReport\n",
      "24/02/12 14:27:55 WARN Utils: Your hostname, sigmoid-20JVA03PIG resolves to a loopback address: 127.0.1.1; using 172.29.170.199 instead (on interface wlp5s0)\n",
      "24/02/12 14:27:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/12 14:27:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m df_spark \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewdata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Generate a profile report\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m profile \u001b[38;5;241m=\u001b[39m ProfileReport(\u001b[43mdf_stats\u001b[49m\u001b[38;5;241m.\u001b[39mtoPandas())\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the profile report as an interactive HTML file\u001b[39;00m\n\u001b[1;32m     14\u001b[0m profile\u001b[38;5;241m.\u001b[39mto_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyspark_report.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_stats' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkProfile\").getOrCreate()\n",
    "\n",
    "# Load dataset into a PySpark DataFrame\n",
    "df_spark = spark.read.csv('newdata.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Generate a profile report\n",
    "profile = ProfileReport(df_stats.toPandas())\n",
    "\n",
    "# Save the profile report as an interactive HTML file\n",
    "profile.to_file(\"pyspark_report.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark_df_profiling\n",
      "  Downloading spark_df_profiling-1.1.13-py2.py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m896.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.8 in ./menv/lib/python3.11/site-packages (from spark_df_profiling) (3.1.3)\n",
      "Requirement already satisfied: matplotlib>=1.4 in ./menv/lib/python3.11/site-packages (from spark_df_profiling) (3.8.2)\n",
      "Requirement already satisfied: pandas>=0.17.0 in ./menv/lib/python3.11/site-packages (from spark_df_profiling) (2.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in ./menv/lib/python3.11/site-packages (from spark_df_profiling) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./menv/lib/python3.11/site-packages (from jinja2>=2.8->spark_df_profiling) (2.1.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./menv/lib/python3.11/site-packages (from matplotlib>=1.4->spark_df_profiling) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./menv/lib/python3.11/site-packages (from matplotlib>=1.4->spark_df_profiling) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./menv/lib/python3.11/site-packages (from matplotlib>=1.4->spark_df_profiling) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./menv/lib/python3.11/site-packages (from matplotlib>=1.4->spark_df_profiling) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in ./menv/lib/python3.11/site-packages (from matplotlib>=1.4->spark_df_profiling) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./menv/lib/python3.11/site-packages (from matplotlib>=1.4->spark_df_profiling) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in ./menv/lib/python3.11/site-packages (from matplotlib>=1.4->spark_df_profiling) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./menv/lib/python3.11/site-packages (from matplotlib>=1.4->spark_df_profiling) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./menv/lib/python3.11/site-packages (from matplotlib>=1.4->spark_df_profiling) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./menv/lib/python3.11/site-packages (from pandas>=0.17.0->spark_df_profiling) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./menv/lib/python3.11/site-packages (from pandas>=0.17.0->spark_df_profiling) (2023.4)\n",
      "Installing collected packages: spark_df_profiling\n",
      "Successfully installed spark_df_profiling-1.1.13\n"
     ]
    }
   ],
   "source": [
    "! pip install spark_df_profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Profiling Metrics:\n",
      "row_count: 10\n",
      "property_count: 4\n",
      "empty_properties: {'Name': 0, 'Age': 0, 'Salary': 0, 'Date_of_joining': 0}\n",
      "average: {'Name': None, 'Age': 37.9, 'Salary': 43316.6}\n",
      "sum: {'Name': None, 'Age': 379, 'Salary': 433166.0}\n",
      "standard_deviation: {'Name': None, 'Age': 11.454741861391339, 'Salary': 11282.762469064628}\n",
      "min: {'Name': 'Craig', 'Age': 23, 'Salary': 24560.0}\n",
      "max: {'Name': 'smith', 'Age': 56, 'Salary': 61000.0}\n",
      "sample_values: {'Name': ['booker', 'grey', 'johnson', 'jenkins', 'smith'], 'Age': [23, 56, 43, 34, 32], 'Salary': [55000.0, 45000.0, 45400.0, 32400.0, 30000.0], 'Date_of_joining': [datetime.date(2022, 2, 1), datetime.date(2023, 7, 7), datetime.date(2022, 3, 11), datetime.date(2019, 8, 16), datetime.date(2020, 10, 4)]}\n"
     ]
    }
   ],
   "source": [
    "# Without using pandas_profiling\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean, stddev, min, max, count, when, col\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySparkProfile\").getOrCreate()\n",
    "\n",
    "df_spark = spark.read.csv('newdata.csv', header=True, inferSchema=True)\n",
    "\n",
    "metrics = {\n",
    "    'row_count': df_spark.count(),\n",
    "    'property_count': len(df_spark.columns),\n",
    "    'empty_properties': {c: df_spark.where(col(c).isNull()).count() for c in df_spark.columns},\n",
    "    'average': {c: df_spark.select(mean(col(c))).first()[0] for c in df_spark.columns if c != \"Date_of_joining\"},\n",
    "    'sum': {c: df_spark.agg({c: \"sum\"}).first()[0] for c in df_spark.columns if c != \"Date_of_joining\"},\n",
    "    'standard_deviation': {c: df_spark.select(stddev(col(c))).first()[0] for c in df_spark.columns if c != \"Date_of_joining\"},\n",
    "    'min': {c: df_spark.select(min(col(c))).first()[0] for c in df_spark.columns if c != \"Date_of_joining\"},\n",
    "    'max': {c: df_spark.select(max(col(c))).first()[0] for c in df_spark.columns if c != \"Date_of_joining\"},\n",
    "    'sample_values': {c: df_spark.select(col(c)).limit(5).rdd.flatMap(lambda x: x).collect() for c in df_spark.columns},\n",
    "\n",
    "}\n",
    "\n",
    "# Generate HTML report\n",
    "html_report = \"<html><head><title>PySpark Data Profile Report</title></head><body>\"\n",
    "\n",
    "# Add metrics to HTML report\n",
    "html_report += \"<h2>Data Profiling Metrics:</h2>\"\n",
    "html_report += \"<table border='1'><tr><th>Metric</th><th>Value</th></tr>\"\n",
    "for metric, value in metrics.items():\n",
    "    if isinstance(value, dict):\n",
    "        value_str = ', '.join(f\"{k}: {v}\" for k, v in value.items())\n",
    "    else:\n",
    "        value_str = str(value)\n",
    "    html_report += f\"<tr><td>{metric}</td><td>{value_str}</td></tr>\"\n",
    "html_report += \"</table>\"\n",
    "\n",
    "# End HTML report\n",
    "html_report += \"</body></html>\"\n",
    "\n",
    "# Save the profile report as an interactive HTML file\n",
    "with open(\"profile_report899.html\", \"w\") as f:\n",
    "    f.write(html_report)\n",
    "\n",
    "# print(\"Data Profiling Metrics:\")\n",
    "# for metric, value in metrics.items():\n",
    "#     print(f\"{metric}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
